{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import Game\n",
    "from random_agent import RandomAgent\n",
    "import numpy as np\n",
    "import torch\n",
    "#import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# The Q-table\n",
    "Q = {}\n",
    "S = {}\n",
    "\n",
    "# Function to choose action based on epsilon-greedy policy\n",
    "def epsilon_greedy_policy(q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(len(q_values))  # Random action\n",
    "    else:\n",
    "        return np.argmax(q_values)  # Greedy action\n",
    "\n",
    "def q_learning_play(game, qtable, states_table):\n",
    "    possible_states = list()\n",
    "    # Make hashes of each possible next game state\n",
    "    for x in range(3):\n",
    "        for y in range(3):\n",
    "            #print(f\"x: {x} | y: {y} | value: {game.board.squares[x][y]}\")\n",
    "            if game.board.squares[x][y] == 0:\n",
    "                game_copy = deepcopy(game)\n",
    "                game_copy.play(x, y)\n",
    "                possible_states.append((x, y, game_copy.board.calculate_hash()))\n",
    "    # Get q_values of all the possible next game states from Q table\n",
    "    # or initialized with 0.5 if the state was never seen before\n",
    "    q_values = list()\n",
    "    for state in possible_states:\n",
    "        state_hash = state[2]\n",
    "        if state_hash in qtable:\n",
    "            q_values.append(qtable[state_hash])\n",
    "        else:\n",
    "            q_values.append(0.5) # our initial Q values\n",
    "            qtable[state_hash] = 0.5\n",
    "    # Use epsilon greedy to make a move\n",
    "    picked_value = epsilon_greedy_policy(q_values, 0.1)\n",
    "    states_table[game.board.calculate_hash()] = possible_states\n",
    "    x, y, _ = possible_states[picked_value]\n",
    "    return(x, y, qtable, states_table)\n",
    "\n",
    "def undo_hash(game_hash):\n",
    "    # Convert the integer hash to a string\n",
    "    game_hash_str = str(game_hash)\n",
    "    # Insert spaces to separate the digits\n",
    "    game_hash_str_spaced = ' '.join(game_hash_str)\n",
    "    # Convert the string back to a numpy array\n",
    "    game_hash_array = np.fromstring(game_hash_str_spaced, dtype=int, sep=' ')\n",
    "    while len(game_hash_array) < 9:\n",
    "        game_hash_array = np.concatenate(([0], game_hash_array))\n",
    "    # Reshape the numpy array to its original shape (3x3)\n",
    "    original_shape = (3, 3)\n",
    "    original_array = np.reshape(game_hash_array, original_shape)\n",
    "    return original_array\n",
    "\n",
    "def extract_max_possible_q(state, Q):\n",
    "    squares = undo_hash(state)\n",
    "    # Setting up the game from the given state\n",
    "    game = Game()\n",
    "    game.board.squares = squares\n",
    "    game.turn = np.sum(game.board.squares != 0)\n",
    "    # Explore possible next states\n",
    "    max_Q = 0\n",
    "    for x in range(3):\n",
    "        for y in range(3):\n",
    "            if game.board.squares[x][y] == 0:\n",
    "                game_copy = deepcopy(game)\n",
    "                game_copy.play(x, y)\n",
    "                possible_state = game_copy.board.calculate_hash()\n",
    "                if possible_state in Q:\n",
    "                    if Q[possible_state] > max_Q:\n",
    "                        max_Q = Q[possible_state]\n",
    "                else:\n",
    "                    Q[possible_state] = 0.5\n",
    "                    if Q[possible_state] > max_Q:\n",
    "                        max_Q = Q[possible_state]\n",
    "    return max_Q\n",
    "\n",
    "# 1. Keep track of all the selected states\n",
    "# 2. The very last state gets the player reward from the game as its value\n",
    "# 3. Step backwards to the 2nd last state. Now identify, from here, of all the next possible states, the\n",
    "#    highest Q-value (make fn for this). This is max_a Q(S', a).\n",
    "# 4. Plug that value into the Q update formula:\n",
    "#    - (1 - learning rate) * the old Q + learning rate * disc fac * max_a Q(S', a)\n",
    "# 5. Repeat backwards through all the states that were selected in the game\n",
    "\n",
    "# put all this inside 100 games e.g.\n",
    "###############################################################################\n",
    "\n",
    "Q = {}\n",
    "rewards = list()\n",
    "gamma = 0.95\n",
    "alpha = 0.95\n",
    "\n",
    "for i in range(10000):\n",
    "    print(f\"Game {i}\")\n",
    "    random_agent = RandomAgent()\n",
    "    visited_states = list()\n",
    "    game = Game()\n",
    "    while not game.done:\n",
    "        # Q-agent is P1\n",
    "        x, y, Q, S = q_learning_play(game, Q, S)\n",
    "        game.play(x, y)\n",
    "        # game.board.show()\n",
    "        visited_state = game.board.calculate_hash()\n",
    "        visited_states.append(visited_state)\n",
    "        if game.done:\n",
    "            break\n",
    "        # Random Agent is P2\n",
    "        random_agent.play(game)\n",
    "        #print(\"Random agent turn:\")\n",
    "        #game.board.show()\n",
    "    # Keep track of q-agent's rewards for our own monitoring\n",
    "    rewards.append(game.p1_reward)\n",
    "    # Update Q table by working backwards through visited states\n",
    "    rev_states = visited_states[::-1]\n",
    "    for i in range(len(rev_states)):\n",
    "        state = rev_states[i]\n",
    "        if i == 0:\n",
    "            Q[state] = game.p1_reward\n",
    "        else:\n",
    "            max_next_q = extract_max_possible_q(state, Q)\n",
    "            Q[state] = ((1 - gamma) * Q[state]) + gamma * alpha * max_next_q\n",
    "\n",
    "Q2 = {}\n",
    "# Check Q-agent performance vs. random agent after 100 games\n",
    "evaluation_rewards = list()\n",
    "for i in range(100):\n",
    "    random_agent = RandomAgent()\n",
    "    game = Game()\n",
    "    while not game.done:\n",
    "        # Q-agent is P1\n",
    "        x, y, Q2, S = q_learning_play(game, Q2, S)\n",
    "        game.play(x, y)\n",
    "        # game.board.show()\n",
    "        if game.done:\n",
    "            break\n",
    "        # Random Agent is P2\n",
    "        random_agent.play(game)\n",
    "    # Keep track of q-agent's rewards for our own monitoring\n",
    "    evaluation_rewards.append(game.p1_reward)\n",
    "np.mean(evaluation_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tictactoe import Board, Game\n",
    "from random_agent import RandomAgent\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(nn.Linear(18, 18), nn.ReLU(), nn.Linear(18, 18), nn.ReLU(), nn.Linear(18, 18), nn.ReLU(), nn.Linear(18, 18), nn.ReLU(), nn.Linear(18, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # x = torch.relu(self.fc1(x))  # Apply ReLU activation to the output of the first layer\n",
    "        # x = self.fc2(x)              # Apply the second linear transformation\n",
    "        return logits\n",
    "    \n",
    "model = TicTacToeNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def epsilon_greedy_policy(q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(len(q_values))  # Random action\n",
    "    else:\n",
    "        return np.argmax(q_values)  # Greedy action\n",
    "\n",
    "\n",
    "def train_data(data, model):\n",
    "    num_epochs = 50\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # output = []\n",
    "        for squares, q in data:\n",
    "            model.train()\n",
    "\n",
    "            predicted_q = model(squares)\n",
    "            target_q = q\n",
    "            \n",
    "            loss = criterion(predicted_q, target_q) # find out why this is working.\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # # Forward pass\n",
    "            # output.append(model(squares))\n",
    "            \n",
    "            # # Compute the loss\n",
    "            # loss = criterion(output, q)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "    # model.train()\n",
    "    # loss = loss_fn(torch.tensor(predictive_qs), torch.tensor(target_qs))\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "rewards = list()\n",
    "gamma = 0.95\n",
    "alpha = 0.95\n",
    "\n",
    "def board_conversion_to_2x9(squares):\n",
    "    x = []\n",
    "    o = []\n",
    "\n",
    "    for i in range(len(squares)):\n",
    "        for j in range(len(squares[i])):\n",
    "            if squares[i][j] == 1:\n",
    "                x.append(1)\n",
    "                o.append(0)\n",
    "            elif squares[i][j] == 2:\n",
    "                x.append(0)\n",
    "                o.append(1)\n",
    "            else:\n",
    "                x.append(0)\n",
    "                o.append(0)\n",
    "    \n",
    "    return torch.tensor(x + o).float()\n",
    "\n",
    "def nn_play(game, model):\n",
    "    possible_states = list()\n",
    "    # Make hashes of each possible next game state\n",
    "    for x in range(3):\n",
    "        for y in range(3):\n",
    "            if game.board.squares[x][y] == 0:\n",
    "                game_copy = deepcopy(game)\n",
    "                game_copy.play(x, y)\n",
    "                possible_states.append((x, y, game_copy.board.squares))\n",
    "    # Get q_values of all the possible next game states from Q table\n",
    "    # or initialized with 0.5 if the state was never seen before\n",
    "    q_values = [model(board_conversion_to_2x9(possible_state[2]).cuda()).detach().cpu().numpy() for possible_state in possible_states]\n",
    "    # Use epsilon greedy to make a move\n",
    "    picked_value = epsilon_greedy_policy(q_values, 0.1)\n",
    "    x, y, _ = possible_states[picked_value]\n",
    "    return(x, y, q_values[picked_value])        \n",
    "\n",
    "def extract_max_possible_q(squares, model):\n",
    "    # Setting up the game from the given state\n",
    "    game = Game()\n",
    "    game.board.squares = squares\n",
    "    game.turn = np.sum(game.board.squares != 0)\n",
    "    # Explore possible next states\n",
    "    max_Q = 0\n",
    "    for x in range(3):\n",
    "        for y in range(3):\n",
    "            if game.board.squares[x][y] == 0:\n",
    "                game_copy = deepcopy(game)\n",
    "                game_copy.play(x, y)\n",
    "                Q_possible_state = model(board_conversion_to_2x9(squares).cuda())\n",
    "                if Q_possible_state > max_Q:\n",
    "                    max_Q = Q_possible_state\n",
    "    return max_Q\n",
    "\n",
    "training_data = []\n",
    "rewards = []\n",
    "\n",
    "win_perc = []\n",
    "\n",
    "for i in range(1001):\n",
    "    random_agent = RandomAgent()\n",
    "    visited_states = list()\n",
    "    game = Game()\n",
    "    while not game.done:\n",
    "        x, y, Q = nn_play(game, model)\n",
    "        game.play(x, y)\n",
    "        visited_state = deepcopy(game.board.squares)\n",
    "        visited_states.append((visited_state, Q))\n",
    "        if game.done:\n",
    "            break\n",
    "        random_agent.play(game)\n",
    "    # Keep track of q-agent's rewards for our own monitoring\n",
    "    rewards.append(game.p1_reward)\n",
    "    # Update Q table by working backwards through visited states\n",
    "    rev_states = visited_states[::-1]\n",
    "    for j in range(len(rev_states)):\n",
    "        state = rev_states[j]\n",
    "        squares = state[0]\n",
    "        if j == 0:\n",
    "            theoretical_Q = game.p1_reward\n",
    "        else:\n",
    "            max_next_q = extract_max_possible_q(squares, model)\n",
    "            theoretical_Q = ((1 - gamma) * theoretical_Q) + gamma * alpha * max_next_q\n",
    "        training_data.append([board_conversion_to_2x9(squares).cuda(), torch.tensor(theoretical_Q, dtype=torch.float).cuda()])\n",
    "\n",
    "    if i % 100 == 0 and i != 0:\n",
    "        # train the training data for the model\n",
    "        # predictive_qs = [model(squares) for squares, qs in training_data]\n",
    "        # target_qs = [qs for squares, qs in training_data]\n",
    "\n",
    "        train_data(training_data, model)\n",
    "\n",
    "        win_count = 0\n",
    "        tie_count = 0\n",
    "        lose_count = 0\n",
    "\n",
    "        recent_rewards = rewards[-100:]\n",
    "        for reward in recent_rewards:\n",
    "            if reward == 1:\n",
    "                win_count += 1\n",
    "            elif reward == 0.5:\n",
    "                tie_count += 1\n",
    "            else:\n",
    "                lose_count += 1\n",
    "\n",
    "        win_perc.append(win_count / 100)\n",
    "\n",
    "        print(\"win % : \", format(win_count / 100, \".0%\"))\n",
    "        print(\"tie % : \", format(tie_count / 100, \".0%\"))\n",
    "        print(\"lose % : \", format(lose_count / 100, \".0%\"))\n",
    "\n",
    "        training_data = []\n",
    "\n",
    "win_numbers = [int(item.strip('%')) for item in win_perc]\n",
    "\n",
    "# data to be plotted\n",
    "x = np.arange(0, len(win_numbers))\n",
    " \n",
    "# plotting\n",
    "plt.plot(x, win_numbers)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
