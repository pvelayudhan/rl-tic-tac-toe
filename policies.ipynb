{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "0uaRz4M2ub"
      },
      "source": [
        "from tictactoe import Game\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "nBwOuE3A7p"
      },
      "source": [
        "game = Game()\n",
        "\n",
        "game.board.show()\n",
        "\n",
        "# Win sequence\n",
        "game.play(0, 2)\n",
        "game.play(1, 2)\n",
        "game.play(0, 1)\n",
        "game.play(2, 2)\n",
        "game.play(1, 1)\n",
        "game.play(2, 1)\n",
        "\n",
        "\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.winner\n",
        "\n",
        "\n",
        "# Draw sequence\n",
        "game.play(0, 2)\n",
        "game.board.squares\n",
        "game.play(1, 1)\n",
        "game.board.squares\n",
        "game.play(0, 1)\n",
        "game.board.squares\n",
        "game.play(0, 0)\n",
        "game.board.squares\n",
        "game.play(2, 2)\n",
        "game.board.squares\n",
        "game.play(1, 2)\n",
        "game.board.squares\n",
        "game.play(1, 0)\n",
        "game.board.squares\n",
        "game.play(2, 1)\n",
        "game.board.squares\n",
        "game.play(2, 0)\n",
        "game.board.squares"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "6rPK8eNKTp"
      },
      "source": [
        "# https://stackoverflow.com/a/38250088/16626788\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def ewanmax(x):\n",
        "    x = np.asarray(x).astype('float64')\n",
        "    return x / np.sum(x)\n",
        "\n",
        "# This returns a legal, soft-max'd move\n",
        "def choose_move(game, square_probs):\n",
        "    legal_moves = np.where(np.reshape(game.squares, [9]) == 0)[0]\n",
        "    legal_move_chosen = False\n",
        "    #soft_probs = softmax(tf.reshape(square_probs, [9]))\n",
        "    soft_probs = ewanmax(tf.reshape(square_probs, [9]))\n",
        "    print(np.sum(soft_probs))\n",
        "    while legal_move_chosen is False:\n",
        "        chosen_move = np.random.choice(np.arange(9), 1, p=soft_probs)[0]\n",
        "        if chosen_move in legal_moves:\n",
        "            legal_move_chosen = True\n",
        "    formatted_move = np.zeros(9, dtype=float)\n",
        "    formatted_move[chosen_move] = 1\n",
        "    formatted_move = tf.convert_to_tensor(formatted_move[np.newaxis])\n",
        "    return formatted_move\n",
        "\n",
        "\n",
        "# Randomly select a legal move\n",
        "def play_random_move(game):\n",
        "    if game.done is True:\n",
        "        return None\n",
        "    legal_moves = np.where(np.reshape(game.squares, [9]) == 0)[0]\n",
        "    legal_move_chosen = False\n",
        "    while legal_move_chosen is False:\n",
        "        chosen_move = np.random.choice(np.arange(9), 1)[0]\n",
        "        if chosen_move in legal_moves:\n",
        "            legal_move_chosen = True\n",
        "    formatted_move = np.zeros(9, dtype=float)\n",
        "    formatted_move[chosen_move] = 1\n",
        "    formatted_move = tf.convert_to_tensor(formatted_move[np.newaxis])\n",
        "    target_square = np.reshape(formatted_move, [3, 3])\n",
        "    rank, file = np.where(target_square == 1)\n",
        "    game.play(rank[0], file[0])\n",
        "    return None\n",
        "\n",
        "\n",
        "def play_one_move(game, model, loss_fn):\n",
        "    if game.done is True:\n",
        "        print(\"cheese\")\n",
        "        return None\n",
        "    with tf.GradientTape() as tape:\n",
        "        # 1. extract a state that can be fed into model()\n",
        "        formatted_board = game.board.squares.reshape(1, 9)\n",
        "        # 2. Get a prediction from model()\n",
        "        square_probs = model(formatted_board)\n",
        "        # 3. Pick an action that can be fed into environment\n",
        "        target_square = choose_move(game, square_probs)\n",
        "        #print(square_probs)\n",
        "        # 4. Define the loss(?)\n",
        "        loss = tf.reduce_mean(loss_fn(target_square, square_probs))\n",
        "    # Calculate the gradients between predicted and actual(?)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    # Actually play the selected move, collecting the reward and done state\n",
        "    target_square = np.reshape(target_square, [3, 3])\n",
        "    rank, file = np.where(target_square == 1)\n",
        "    game.play(rank[0], file[0])\n",
        "    return grads"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "SzXI15gBzx"
      },
      "source": [
        "loss_fn = tf.keras.losses.categorical_crossentropy"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "1sGtfgIBy2"
      },
      "source": [
        "# Each step = one turn of tic-tac-toe\n",
        "# Each \"episode\" = one whole game of tic-tac-toe\n",
        "def play_multiple_episodes(n_episodes, model, loss_fn, n_max_steps=9):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    #1. Loop through many games of ttt\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        #2. Initialize a new game\n",
        "        #print(f\"Playing game #{episode}\")\n",
        "        game = Game()\n",
        "        # Win sequence\n",
        "        game.play(0, 2)\n",
        "        game.play(1, 2)\n",
        "        game.play(0, 1)\n",
        "        game.play(2, 2)\n",
        "        game.play(1, 1)\n",
        "        game.play(2, 1)\n",
        "        for i in range(n_max_steps):\n",
        "            grads = play_one_move(game, model, loss_fn)\n",
        "            play_random_move(game)\n",
        "            reward = game.p1_reward\n",
        "            done = game.done\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done:\n",
        "                break\n",
        "        print(episode)\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    # all_rewards is a list containing a list of rewards for each episode\n",
        "    # all_grads is a list containing a list of gradients for each episode\n",
        "    return all_rewards, all_grads\n",
        "\n",
        "# Player 1 wins on their turn\n",
        "# Player 1 draws on their turn\n",
        "# Player 2 wins on their turn\n",
        "# Player 2 draws on their turn\n",
        "# To-do: figure out the reward logic\n",
        "\n",
        "# A discounted reward is the reward for the current step plus the reward for\n",
        "#  all future steps multiplied by the disocunt factor. For example, if our\n",
        "#  rewards were [2, 4, 8] and we had a discount factor of 0.5, our discounted\n",
        "#  rewards would be [(2 + 8*0.5 =) 6, (4 + 8*0.5 =) 8, 8]. Note that this\n",
        "#  calculation is done starting at the last reward and working backwards.\n",
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    # Range params are start index, end index, and step size\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discount_factor * discounted[step + 1]\n",
        "    return discounted\n",
        "\n",
        "\n",
        "# This function performs standard normalization across ALL rewards\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    # This can cause division by zero error\n",
        "    #print(\"apple\")\n",
        "    #print(reward_std)\n",
        "    #print(\"banana\")\n",
        "    return all_discounted_rewards\n",
        "    #return [(discounted_rewards - reward_mean) / reward_std\n",
        "    #        for discounted_rewards in all_discounted_rewards]\n",
        "\n",
        "\n",
        "\n",
        "#n_iterations = 150\n",
        "#n_episodes_per_update = 20\n",
        "#discount_factor = 0.95\n",
        "\n",
        "\n",
        "n_iterations = 100\n",
        "n_episodes_per_update = 10\n",
        "discount_factor = 0.95"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "kbUk7tZ9l2"
      },
      "source": [
        "random.seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(9, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
        "\n",
        "plot_rewards = []\n",
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(n_episodes_per_update, model, loss_fn)\n",
        "    # extra code \u2013 displays some debug info during training\n",
        "    total_rewards = sum(map(sum, all_rewards))\n",
        "    plot_rewards.append(total_rewards / n_episodes_per_update) \n",
        "    print(f\"\\rIteration: {iteration + 1}/{n_iterations},\"\n",
        "          f\" mean rewards: {total_rewards / n_episodes_per_update:.1f}\", end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
        "                                                       discount_factor)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
        "\n",
        "a = np.array(plot_rewards) / n_episodes_per_update\n",
        "\n",
        "plt.plot(a)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "TmfE7N4eBV"
      },
      "source": [
        "game = Game()\n",
        "game.play(0, 2)\n",
        "game.play(1, 2)\n",
        "game.play(0, 1)\n",
        "game.play(2, 2)\n",
        "game.play(1, 1)\n",
        "game.play(2, 1)\n",
        "\n",
        "\n",
        "\n",
        "model_guesses = model(game.board.squares.reshape(1, 9))\n",
        "\n",
        "np.reshape(model_guesses, [3, 3])\n",
        "\n",
        "\n",
        "trained_model_guesses = model(game.board.squares.reshape(1, 9))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.reshape(trained_model_guesses, [3, 3])\n",
        "\n",
        "guess = tf.reshape(trained_model_guesses, [9])\n",
        "\n",
        "guess / np.sum(guess)\n",
        "\n",
        "np.reshape(softmax(tf.reshape(trained_model_guesses, [9])), [3, 3])\n",
        "\n",
        "\n",
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(9, activation=\"sigmoid\")\n",
        "])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "od7Xv8cOj1"
      },
      "source": [
        "game = Game()\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_one_move(game, model, loss_fn)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_random_move(game)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_one_move(game, model, loss_fn)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_random_move(game)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_one_move(game, model, loss_fn)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_random_move(game)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_one_move(game, model, loss_fn)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_random_move(game)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "play_one_move(game, model, loss_fn)\n",
        "game.p1_reward\n",
        "game.done\n",
        "game.board.show()\n",
        "\n",
        "\n",
        "game.play(0, 2)\n",
        "\n",
        "\n",
        "play_one_move(game, model, loss_fn)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}